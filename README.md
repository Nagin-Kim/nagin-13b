# nagin-13b
### 我们发布的nagin-13B采用自研transformer和改进的adapter结构设计，通过在2T左右的图书数据上我们进行了早期阶段的重新预训练，并在2000w条sft数据集（包括开源和自收集）上进行了第二阶段的微调训练，最后，针对推理能力和数学能力不足的问题，我们收集了大概50w条包括c-eval，gaokao，cmmlu，mmlu，信通院测试数据样例以及部分开放教育题库在内的题库数据上，进行了第三阶段的打榜微调训练，模型将于近期整理开源到modelspace或huggingface。此外，目前结合了工信部AIGC可信标准，正在收集更多价值观、偏见、种族歧视数据指令进行第四阶段的训练，以此增加了模型的安全输出控制。
